{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target y can take on more than two possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generalization of a logistic regression model to the case where we want to handle multiple classes. \n",
    "\n",
    "- Example: softmax regression model for a 4-class classification problem (4 possible values for y).\n",
    "\n",
    "$n = 4$\n",
    "\n",
    "$z_n = \\vec{w_n}\\cdot x + b_n$\n",
    "\n",
    "**softmax formula**\n",
    "\n",
    "$a_1 = \\frac{e^z1}{e^z1 + e^z2 + e^z3 + e^z4}  = P(y= 1|\\vec{x})$\n",
    "\n",
    "$a_2 = \\frac{e^z2}{e^z1 + e^z2 + e^z3 + e^z4} = P(y= 2|\\vec{x})$ \n",
    "\n",
    "$\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General formula:**\n",
    "\n",
    "$z_n = \\vec{w_n}\\cdot x + b_n$, y = 1, 2, ..., N\n",
    "\n",
    "$a_j = \\frac{e^zj}{\\sum_{k=1}^N e^zk} = P(y= j|\\vec{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for softmax regression is the average of the loss function for each training example plus a regularization term:\n",
    "\n",
    "$loss(a_1, a_2, ..., a_N, y) = -\\sum_{j=1}^N \\log a_N if y=N$ \n",
    "\n",
    "This is called the **cross-entropy** loss. This incentivates the algorithm to increase the probability of the correct class for each training example. To maximize $a_N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Softmax Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with classification of ten handwritten digits, we have 10 output neurons, one for each possible digit. The output of the softmax function for each of these neurons is the probability that the input image is that digit. The cost function is the average of the cross-entropy loss for each training example plus a regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.SparseCategoricalCrossentropy()) \n",
    "\n",
    "# Sparse refers to the fact that each digit can only be classifies in one class.\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Implementation of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More numerical accurate implementation of logistic loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intead of this\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossEntropy())\n",
    "\n",
    "# We can do this\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossEntropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This lets the algorithm to decide how to compute the loss function. It replaces \"a\" directly in the loss function with \"z\" and \"z\" is computed directly from the input data. This is more numerically stable than computing \"a\" first and then computing \"z\" from \"a\".\n",
    "\n",
    "- This allows Tensorflow to have a little bit less numerical roundoff error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the whole code will be\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "#Loss\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossEntropy(from_logits=True)) \n",
    "\n",
    "# Fit the model\n",
    "\n",
    "model.fit(X, Y, epochs=100)\n",
    "\n",
    "#Predictions\n",
    "\n",
    "logit = model(X)\n",
    "\n",
    "f_x = tf.nn.sigmoid(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Softmax regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of this \n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.SparseCategoricalCrossEntropy())\n",
    "\n",
    "# We can do this\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.SparseCategoricalCrossEntropy(from_logits=True))\n",
    "\n",
    "# More numerical accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the whole code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "#Loss\n",
    "\n",
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True)) \n",
    "\n",
    "# Fit the model\n",
    "\n",
    "model.fit(X, Y, epochs=100)\n",
    "\n",
    "#Predictions\n",
    "\n",
    "logits = model(X)\n",
    "\n",
    "f_x = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-label classification problem is where associate to each image, there are multiple labels. For example, an image of a dog and a cat, the labels are \"dog\" and \"cat\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement a neural network for multi-label classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option One:**\n",
    "\n",
    "Treat the situation as two completely separate machine learning problems. One is a binary classification problem for the dog and the other is a binary classification problem for the cat.\n",
    "\n",
    "**Option Two:**\n",
    "\n",
    "Train a single neural network simultaneously to predict both the dog and the cat. For this we need to change the output layer of the neural network. It would have to have two output neurons, one for the dog and one for the cat. \n",
    "\n",
    "As the two separate probles are binary classification problems, we can use the sigmoid function as the activation function for the output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

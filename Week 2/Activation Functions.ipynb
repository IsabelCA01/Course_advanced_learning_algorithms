{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most commonly used activation functions\n",
    "\n",
    "- **sigmoid function**: it is a function that maps any value to a value between 0 and 1. It is a non-linear function used in the output layer of a neural network to predict the probability of an input belonging to a certain class. It is also used in the hidden layers of a neural network to capture non-linearities. It is given by the following equation:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- **ReLU**: (Rectified Linear Unit) it is a function that maps any value to a value between 0 and infinity (only positive values). It is a non-linear function used in the hidden layers of a neural network to capture non-linearities. It is given by the following equation:\n",
    "\n",
    "$$ReLU(x) = max(0,x)$$\n",
    "\n",
    "- **Linear activation function**: it is a function that maps any value to itself. It is a linear function used in the output layer of a neural network to predict a continuous value. Sometimes people say, we're not using an activation function. It is given by the following equation:\n",
    "\n",
    "$$f(x) = x$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "\n",
    "It depends on the target label or the ground truth label y is. \n",
    "\n",
    "- If y is a binary label, then we use sigmoid function in the output layer. Because the algorithm learns to predict the probability of 1.\n",
    "\n",
    "- If y is a multi-class label, then we use softmax function in the output layer. Because the algorithm learns to predict the probability of each class.\n",
    "\n",
    "- If y is a continuous value, then we use linear activation function in the output layer. Because the algorithm learns to predict a continuous value. Y can be any number positive or negative.\n",
    "\n",
    "- If y is a positive continuous value, then we use ReLU activation function in the output layer. Because the algorithm learns to predict a positive continuous value. Y can be any number positive or zero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "\n",
    "- ReLU is the most common choice. Why? Because it is a non-linear function and it is easy to compute.\n",
    "\n",
    "- Sigmoid is slower. And it goes flat on two sides. ReLU only goes flat on the negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(15, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') #or \"linear\", \"softmax\", \"tanh\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other activation functions\n",
    "\n",
    "- LeakyReLU\n",
    "\n",
    "- GeLU\n",
    "\n",
    "- ELU\n",
    "\n",
    "- SELU\n",
    "\n",
    "- Swish\n",
    "\n",
    "- Softplus\n",
    "\n",
    "- Softsign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need activation functions?\n",
    "\n",
    "- Without activation functions or using linear activation function in all neurons, the neural network is just a linear regression model. No point in using the neural network.\n",
    "\n",
    "- Activation functions introduce non-linearities to the neural network. Without non-linearities, the neural network is just a linear regression model.\n",
    "\n",
    "- A model that uses a linear activation function in all hidden neurons and a sigmoid activation function in the output neuron is equivalent to a logistic regression model.\n",
    "\n",
    "- **Don't use the linear activation in hidden layers**. Use ReLU instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

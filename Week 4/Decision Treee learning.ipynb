{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_1 =$ fraction of examples that are cats\n",
    "\n",
    "The impurity of a set of examples is measured through **entropy**:\n",
    "\n",
    "$H (p_1)$\n",
    "\n",
    "$p_0$ = fraction of examples that are not $p_1$.\n",
    "\n",
    "$p_0 = 1 - p_1$\n",
    "\n",
    "**Actual function**:\n",
    "\n",
    "- $H(p_1) = -p_1 \\log_2 p_1 - p_0 \\log_2 p_0$\n",
    "\n",
    "- $H(p_1) = -p_1 \\log_2 p_1 - (1 - p_1) \\log_2 (1 - p_1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a split: Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide what feature to split on at a node will be based on what choice feature reduces entropy the most.\n",
    "\n",
    "- The reduction of entropy is called **information gain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to combine the left and right branch entropy is called: **weighted average**.\n",
    "\n",
    "- $H_{weighted} = \\frac{N_{left}}{N_{left} + N_{right}} H_{left} + \\frac{N_{right}}{N_{left} + N_{right}} H_{right}$\n",
    "\n",
    "**Information gain formula:**\n",
    "\n",
    "- $IG = H(parent) - H_{weighted}(children)$\n",
    "\n",
    "H(parent) = entropy of the parent node (how many of the examples are cats/ the total number of examples)\n",
    "\n",
    "The higher the information gain, the better the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it toghether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree learning:**\n",
    "\n",
    "1. Start with all examples at the root node\n",
    "\n",
    "2. Calculate information gain for all possible features, and pick the one with the highest information gain.\n",
    "\n",
    "3. Split the dataset according to selected feature, and create left and right branches.\n",
    "\n",
    "4. Repeat steps 1-3 on each branch, until stopping criteria is met:\n",
    "\n",
    "- When a node is 100% pure (all examples belong to the same class)\n",
    "\n",
    "- When splitting a node will result in the tree exceeding a maximum depth.\n",
    "\n",
    "- Information gain from additional splits is less than a threshold.\n",
    "\n",
    "- The number of examples is less than a threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive algorithm:**\n",
    "\n",
    "- A recursive algorithm is an algorithm that calls itself on a smaller version of the input until it reaches a base case.\n",
    "\n",
    "- In decision trees is to build the root base and then build other smaller decision trees on each branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using one-hot encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is for when you have features that have more than two possible values. It creates a new column for each possible value, and turns it into a binary feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous valued features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider many possible thresholds and pick the one that gives the highest information gain."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

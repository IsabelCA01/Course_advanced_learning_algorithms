{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the weaknesses of using a single decision tree is that a single tree can be highly sensitive to small changes. One solution is to make multiple smaller trees, and then average their predictions. This is called a **random forest**. Another solution is to use a **boosting** algorithm, which is a series of trees that each try to correct the mistakes of the previous trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling with replacement is a technique where we randomly select a row from a data set, and then put it back. This means that we can select the same row multiple times. This is useful when we want to randomly select rows, but still want to be able to select the same row multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an algorithm that works much better than using a single decision tree. It works by averaging the predictions of many decision trees. It uses a technique called **bagging**. \n",
    "\n",
    "Bagging is when we create random subsets of the data, and then combine the subsets to make predictions. We can use bagging to create a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a boosting algorithm. It works by having a series of models that each try to correct the mistakes of the previous models. It is a very popular algorithm, and has been used to win many Kaggle competitions.\n",
    "\n",
    "- Use sampling with replacement to create random subsets of the data. But instead of picking from all examples with equal probability, make it more likely to pick missclasified examples from previous trained trees.\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "- open source implementation of boosted trees\n",
    "\n",
    "- fast efficient implementation\n",
    "\n",
    "- Good choice of default splitting criteria for when to stop growing a tree\n",
    "\n",
    "- Built in regularization to prevent overfitting\n",
    "\n",
    "- Highly competitive algorithm for machine learning competitions (eg: Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees vs. Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees and Tree ensembles**\n",
    "\n",
    "- Works well on tabular (structured) data.\n",
    "\n",
    "- Not recommended for unstructured data ( eg: images, audio, text)\n",
    "\n",
    "- Fast to train\n",
    "\n",
    "- Small decision trees may be human interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks**\n",
    "\n",
    "- Works well on all type of data (structured and unstructured).\n",
    "\n",
    "- Slow to train.\n",
    "\n",
    "- Works with transfer learning.\n",
    "\n",
    "- When building a system of multiple models working together, it might be easier to string multiple neural networks together. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

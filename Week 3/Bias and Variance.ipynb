{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosing bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High bias - underfitting\n",
    "\n",
    "- High variance - overfitting\n",
    "\n",
    "\n",
    "To find out if your model has high bias or high variance, you can look at the performance of the training set and the dev set.\n",
    "\n",
    "- High bias: both $J_{train}$ and $J_{cv}$ will be high.\n",
    "\n",
    "- High variance: $J_{train}$ will be low and $J_{cv}$ will be much greater than $J_{train}$.\n",
    "\n",
    "- Just right: both $J_{train}$ and $J_{cv}$ will be low but $J_{cv}$ will be greater than $J_{train}$ by a small amount.\n",
    "\n",
    "- High bias AND high variance: $J_{train}$ will be high and $J_{cv}$ will be much greater than $J_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very large $\\lambda$:**\n",
    " \n",
    "- Here, the very large $\\lambda$ is going to make the $w$'s parameters very small (because we are minimizing the cost function with the regularization term). So the $f_{\\vec w,b}(\\vec x)\\approx b$, which is a very simple function (almost a straight line). So we are going to have a high bias (underfitting).\n",
    "\n",
    "- High bias (underfit)\n",
    "\n",
    "**Very small $\\lambda$:**\n",
    "\n",
    "- Here, the very small $\\lambda$ is going to make the $w$'s parameters very large (because we are minimizing the cost function with the regularization term). So the $f_{\\vec w,b}(\\vec x)$ is going to be a very complex function (almost a wiggly line). So we are going to have a high variance (overfitting).\n",
    "\n",
    "- High variance (overfit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the regularization parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values of $\\lambda$ on a grid. Then, pick the value of $\\lambda$ that has the lowest $J_{cv}(\\vec w,b)$.\n",
    "\n",
    "Then evaluate the model on the test set to make sure it is not overfitting the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing a baseline level of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the level of error you can reasonably hope to get to?\n",
    "\n",
    "- Human level performance\n",
    "\n",
    "- Competing algorithms performance\n",
    "\n",
    "- Guess based on experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Calculate all percentages of error (Baseline performance, Training error and validation error). Then calculate the gap between Baseline performance and Training error and between Training and validation error. If the gap between Baseline performance and Training error is small, then you have a high bias problem. If the gap between Training and validation error is large, then you have a high variance problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves are plots of $J_{train}$ and $J_{cv}$ as a function of the number of training examples.\n",
    "\n",
    "- y axis: error (both $J_{train}$ and $J_{cv}$)\n",
    "\n",
    "- x axis: number of training examples\n",
    "\n",
    "**$J_{train}$**\n",
    "\n",
    "- When there are just a few training examples, the model can fit the training set very well (low error).\n",
    "\n",
    "- As the number of training examples increases, the model cannot fit the training set as well (higher error).\n",
    "\n",
    "- So $J_{train}$ will increase as the number of training examples increases.\n",
    "\n",
    "**$J_{cv}$**\n",
    "\n",
    "- When there are just a few training examples, the model cannot generalize well, so $J_{cv}$ will be high.\n",
    "\n",
    "- As the number of training examples increases, the model can generalize better, so $J_{cv}$ will decrease.\n",
    "\n",
    "- So $J_{cv}$ will decrease as the number of training examples increases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High bias**\n",
    "\n",
    "- When the model has a high bias, $J_{train}$ will get higher as the number of examples increases. But it will flatten out at some point (plateu). The $J_{cv}$ will get lower as the number of examples increases. But it will flatten out at some point (plateu).\n",
    "\n",
    "- The model cannot improve beyond a certain point, no matter how many more examples you feed it.\n",
    "\n",
    "- If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.\n",
    "\n",
    "- To fix this, you need to use a more sophisticated model (one with more parameters).\n",
    "\n",
    "**High variance**\n",
    "\n",
    "- When the model has a high variance, $J_{train}$ will get higher as the number of examples increases. But it will flatten out at some point (plateu). The $J_{cv}$ will get lower as the number of examples increases. But it will flatten out at some point (plateu).\n",
    "\n",
    "- The baseline performance might be higher than the training error. This is because the model is overfitting the training set.\n",
    "\n",
    "- The model can improve with more data, but you need to feed it a lot of data to see any improvement.\n",
    "\n",
    "- If a learning algorithm is suffering from high variance, getting more training data is likely to help.\n",
    "\n",
    "- To fix this, you need to use a simpler model (one with fewer parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding what to do next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For high bias:**\n",
    "\n",
    "- Try decreasing $\\lambda$.\n",
    "\n",
    "- Try adding more features.\n",
    "\n",
    "- Try adding polynomial features ($x_1^2, x_2^2, x_1x_2, \\dots$)\n",
    "\n",
    "\n",
    "**For high variance:**\n",
    "\n",
    "- Get more training data.\n",
    "\n",
    "- Try increasing $\\lambda$.\n",
    "\n",
    "- Try smaller sets of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bias-variance tradeoff: to find a balance between bias and variance. To find the best possible outcome.\n",
    "\n",
    "**Neural networks**\n",
    "\n",
    "- If you make your neural network large enough, you can almost always fit your training set well. But the larger the neural network, the more prone it is to overfitting. That is why you need to regularize properly to avoid overfitting.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Does it do well on the trainig set? \n",
    "\n",
    "- if no: bigger network to reduce high bias.\n",
    "\n",
    "- if yes: go to step 2.\n",
    "\n",
    "2. Does it do well on the dev set?\n",
    "\n",
    "- If no: there's high variance. Get more data.\n",
    "\n",
    "- If yes: Done!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "A large neural network will usually do as well or better than a smaller one, so long as regularization is chosen appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unregularized MNIST model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "layer_1 = Dense(units=25, activation=\"relu\")\n",
    "layer_2 = Dense(units=15, activation=\"relu\")\n",
    "layer_3 = Dense(units=1, activation=\"sigmoid\")\n",
    "model=Sequential([layer_1, layer_2, layer_3])\n",
    "\n",
    "# Regularized MNIST model\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "layer_1 = Dense(units=25, activation=\"relu\", kernel_regularizer=L2(0.01)) # that's the value of lambda\n",
    "layer_2 = Dense(units=15, activation=\"relu\", kernel_regularizer=L2(0.01))\n",
    "layer_3 = Dense(units=1, activation=\"sigmoid\", kernel_regularizer=L2(0.01))\n",
    "model=Sequential([layer_1, layer_2, layer_3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
